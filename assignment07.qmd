---
title: "Assignment07"
author: "Benjamin Reese"
format: pdf
self-contained: true
---

# Exercise 01

## Runing Code to Create Dataset

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(lubridate)
library(tidymodels)

# use this url to download the data directly into R
df <- read_csv("https://data.cityofnewyork.us/api/views/43nn-pn8j/rows.csv")

# clean names with janitor
sampled_df <- df %>% 
  janitor::clean_names() 

# create an inspection year variable
sampled_df <- sampled_df %>%
  mutate(inspection_date = mdy(inspection_date)) %>%
  mutate(inspection_year = year(inspection_date))

# get most-recent inspection
sampled_df <- sampled_df %>%
  group_by(camis) %>%
  filter(inspection_date == max(inspection_date)) %>%
  ungroup()

# subset the data
sampled_df <- sampled_df %>%
  select(camis, boro, zipcode, cuisine_description, inspection_date,
         action, violation_code, violation_description, grade,
         inspection_type, latitude, longitude, council_district,
         census_tract, inspection_year, critical_flag) %>%
  filter(complete.cases(.)) %>%
  filter(inspection_year >= 2017) %>%
  filter(grade %in% c("A", "B", "C")) 

# create the binary target variable
sampled_df <- sampled_df %>%
  mutate(grade = if_else(grade == "A", "A", "Not A")) %>%
  mutate(grade = as.factor(grade))

# create extra predictors
sampled_df <- sampled_df %>%
  group_by(boro, zipcode, cuisine_description, inspection_date,
           action, violation_code, violation_description, grade,
           inspection_type, latitude, longitude, council_district,
           census_tract, inspection_year)  %>%
  mutate(vermin = str_detect(violation_description, pattern = "mice|rats|vermin|roaches")) %>%
  summarize(violations = n(),
            vermin_types = sum(vermin),
            critical_flags = sum(critical_flag == "Y")) %>%
  ungroup()

# write the data
write_csv(sampled_df, "restaurant_grades.csv")

```


## 1. 

In this exercise, I split `sampled_df`, the restaurant data, into training and testing datasets, create a recipe, and estimate a decision tree model.

### a.

```{r}
## Setting Seed
set.seed(20201020)

## Splitting the Sample
split <- initial_split(sampled_df, prop = 0.8)

## Training and Testing
restaurant_train <- training(split)
restaurant_test <- testing(split)
```

### b.

```{r}
## Creating the Recipe
restaurant_rec <- 
  recipe(grade ~ ., data = restaurant_train) %>%
  themis::step_downsample(grade)
```

### c. 

```{r}
## Creating the CART Model
cart_mod <-
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")

## Workflow
cart_wf <- workflow() %>%
  add_recipe(restaurant_rec) %>%
  add_model(cart_mod)

## Estimating the Model
cart_fit <- cart_wf %>%
  fit(data = restaurant_train)
```

## 2.

The code below evaluates the model by generating a confusion matrix and calculates precision and sensitivey. A written description of the quality of the model is also included. 

```{r}
## Selecting the Predictions
predictions <- bind_cols(
  restaurant_test,
  predict(object = cart_fit, new_data = restaurant_test),
  predict(object = cart_fit, new_data = restaurant_test, type = "prob")
) %>%
  mutate(color=as.factor(grade))

## The Predictions
select(predictions, grade, starts_with(".pred"))
```

### a. 

```{r}
## Confusion matrix
conf_mat(data = predictions, truth = grade, estimate = .pred_class)
```

### b.

```{r}
## Precision
precision(data = predictions, truth = grade, estimate = .pred_class)

## Recall/Sensitivity
recall(data = predictions, truth = grade, estimate = .pred_class)

```

### c.

The model's precision, or how often the classifier is correct when it predicts an event, is very high, .998. This high level of precision means that the model is very good at predicting an event when there is an event. In this case, our models is highly precise when predicting a restaurant has an A rating when it actually has an A rating. The model's sensitivity, or how often the classifier is correct when there is an event is not as high as its precision, meaning that there are many restaurants that have an A rating, but the model is predicting that they do not have an A rating. Overall, the quality of the model depends on its intended use and the relative costs associated with its use. If the model predicts an A rating, then we can be pretty sure the result is not a false positive, but we will see many false negatives. 

## 3. 

Using a *KNN*, or K-nearest neighbors, algorithm instead of the decision tree algorithm may improve the quality of the model. Instead of dividing the data into two or more sets as the decision tree model does, a *KNN* algorithm stores all available cases and classifies new cases by taking a majority vote of its k neighbors. This majority vote may prove more accurate than the binary dividing of the decision tree model.

