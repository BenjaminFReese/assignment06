---
title: "assignment06"
author: "Benjamin Reese"
format: html
self-contained: true
---

## Exercise 01: Calculate MSE, RMSE, and MAE.

The following work calculates the MSE, RMSE, and MAE for this table:

true_value    predicted_value      
-------       ------  
     1          2       
     2          2       
     3          4
     4          8
     5          4

First, mean squared error, or MSE, is calculated by the equation: 
$$\hat{MSE} = \frac{1}{n}\sum^{n}_{i=1}(Y_{i}-\hat{Y_{i}})^2$$
Starting with the first row, $(1-2)^2=1$, $(2-2)^2=0$, $(3-4)^2=1$, $(4-8)^2=16$, and $(5-4)^2=1$.
Adding up all of the subtractions gives us, $1+0+1+16+1=19$. The final step is to divide by the sample size, for us that is $n=5$. So, $19/5$, or, approximately, $3.8$ is the mean squared error.

$$\hat{MSE}=\frac{19}{5}\approxeq3.8$$

The root mean square error, or RMSE, is calculated by taking the square root of the MSE. Here is the formula:
$$\hat{RMSE} = \sqrt{\frac{1}{n}\sum^{n}_{i=1}(Y_{i}-\hat{Y_{i}})^2}$$

Taking the square root of our MSE gives us, $\sqrt{3.8}\approxeq1.949359$. The Root Mean Square Error is $1.949359$.

$$\hat{RMSE}=\sqrt{3.8}\approxeq1.949259$$

Finally, the mean absolute error, or MAE, is determined by the formula:
$$\hat{MAE} = \frac{1}{n}\sum^{n}_{i=1}|Y_{i}-\hat{Y_{i}}|^2$$
This formula is very similar to the MSE and RMSE formulas, except we are finding the absolute values of our errors, and not the squared values. The MAE for the data in the table above are calculated as $|1-2|=1$, $|2-2|=0$, $|3-4|=1$, $|4-8|=4$, and $|5-4|=1$. Summing over all of these absolute values gives us: $1+0+1+4+1=7$. Now we just have to divide by our sample size, $n=5$, and we have our MAE. $7/5=1.4$. So our MAE is $7/5$, or, approximately, $1.4$.

$$\hat{MAE}=\frac{7}{5}\approxeq1.4$$

While all three of these equations tell us the error in predictions, they handle outliers differently. The mean squared error method, and by extension, the root mean square error method, is greatly influence by outliers because the differences are squared. MAE can be interpreted as the absolute distances between true and predicted values while mean square error calculates, by squaring, weights higher errors compared to lower errors. In sum, the RMSE error method is more influenced by outliers than mean absolute error.

## Exercise 02: Creating A Binary Classification Confusion Matrix

Below find my work for exercise 02. I start by replicating the table before creating the confusion matrix and calculating precision, sensitivity, and accuracy.

```{r}
knitr::include_graphics("images/ex02.pdf")
```

The key metrics are reported below:

- Accuracy: $\frac{7}{10}=.7$
- Precision: $\frac{3}{4}=.75$
- Recall/Sensitivity: $\frac{3}{5}=.6$

## Exercise 03: Creating A Multiclass Classification Confusion Matrix

Below find my work for exercise 03. Like exercise 02, I start by replicating the table, creating the confusion matrix, classifying each prediction, and then calculating accuracy and the misclassification rate. Accuracy is the proportion of correct predictions divided by the total, found by summing the main diagonal and dividing by the total. The misclassification rate is the proportion of incorrect predictions. We can find that by subtracting accuracy from 1, or $1-Accuracy$. The key results are reported below:

```{r}
knitr::include_graphics("images/ex03.pdf")
```


- Accuracy: $\frac{10}{15}=\frac{2}{3}\approxeq.6667$
- Misclassification Rate: $\frac{5}{15}=\frac{1}{3}\approxeq.3333$

## Exercise 04: Guessing & Predicting

The analytically solution to this problem can be found by multiple the probability that the true value is 0 by the probability that you guessed 0 added to the probability that the true value is 1 times the probability that you guess 1. More formally, $$P(value=0)*P(guess=0) + P(value=1)*P(guess=0)$$. For the first problem, we can simply plugin out probabilities and solve. $$P(value=0) = .49,\  P(guess=0) = .49, \ P(value=1) = .51, \ P(guess=1) = .51$$

$$(.49*.49) + (.51*.51)=0.5002 $$
Approximately, the highest level of accuracy that we can achieve is 50%, or $0.5002$. This means it does not really matter what number that we guess because we have basically a 50% chance that we would be correct.

For the second one, we can also just plug in values to our formula. $$P(value=0) = .99,\ P(guess=0) = .99,\  P(value=1) = .01, \ P(guess=1) = .01$$. Which gives us the following equation:

$$(.99*.99) + (.01*.01)=0.9802 $$
Approximately, the highest level of accuracy that we can achieve is 98%, or $.9802$. Unlike the previous problem, this is a high level of accuracy. We should (almost) always predict 0 because we will be correct 98% of the time. It follows then, that, if we predict $value=1$, we will be correct about 2% of the time.

Context is important when comparing calculated accuracy in machine learning contexts because of relative costs. Depending on situational factors, true positives, true negatives, false positives, and false negatives can have varying levels of severity that must be considered by researchers, perhaps before even testing the model. If a medical test, such as a test for cancer, results in a false negative, than the patient could have diagnosed cancer that severely decreases their survivability. A false positive, on the other hand, may just lead to a few more tests. In that scenario, false negatives are far more consequential than false positives. In that case, we should probably be concerned about sensitivity over accuracy.

There could be other instances, such as detecting nuclear radiation, where a false positive could result in mass evacuations, and the accompanying social hardship, where there was actually no dangerous levels of radiation. In that instance, we would be more interested in precision. 

We have several metrics related to the quality of our tests, accuracy, precision, sensitivity, and specificity, and they are each qualitatively different and should be used as a metric of quality at the determination of the researcher based on expertise and substantive knowledge of relative costs.


## Exercise 05: The Marble Bag Problem

```{r, warning=FALSE, message=FALSE}
## Packages for Exercise05
library(readr)
library(tidymodels)
library(tidyverse)
## Loading in Data
marbles <- read_csv("data/marbles.csv")
```

### 1. Dividing the Marbles Dataset into Training and Testing Data
```{r}
## Setting the seed
set.seed(20200229)

## Splitting
split <- initial_split(data = marbles, prop = .8)

## Training and Testing
marbles_train <- training(x = split)
marbles_test <- testing(x = split)
```

### 2. Developing Mental Model for Predicting Marbles

The chart, tibble, and probabilities below suggest that we should guess that a marble is black if it is big and white if it is small.

```{r}
## Basic Barplot
marbles_train %>%
  ggplot(aes(x=size, fill=color)) +
  geom_bar(color="black") +
  scale_fill_manual(values=c("black", "white")) +
  theme_minimal() +
  labs(x="Marble Size", y="Number of Marbles", 
       title = "The Size and Color of Marbles in A Bag",
       subtitle = "Most Big Marbles are Black")

## Counting Marbles By Size and Color
marbles_train %>%
  count(color, size, sort = TRUE)
```

As shown in the barplot and tibble above, more of the big marbles are black than white, and more of the small marbles are white than black. Therefore, if we reach into a bag and feel that the marble is big, we should guess that it is black, and, if the marble is small, we should guess that it is white. Since, though, there are relatively few small black marbles and relatively more big black marbles, we should be quite accurate predicting a marble is black given it is big. Further, the number of small black marbles is so few that we should be even more accurate when guessing that a small marble is white. 

Analytically, we can think of this problem in terms of conditional probabilities. Conditional probability is defined by the formula:

$$P(A|B) = P(Aâˆ©B) / P(B)$$

We can think of this problem, and model our intuitive results, as wanting to know the conditional probability of a marble being black given it is big, and a marble being white given that it is small. If the conditional probability of a marble being white given it is small is larger than the conditional probability of a marble being black given it is small, then we should predict small marbles will be white. Similarly, if the conditional probability of a marble being black given it is big is larger than the conditional probability of a marble being black given it was small, then we should predict large marbles will be black. The code below uses the tibble above to construct the probabilities and shows that the expected conditional probabilities, derived from the figure above, are consistent with our intuitive predictions.

```{r}
## Probability of white AND small
p_white_small <- 27/100

## Probability of small
p_small <- 37/100

## Conditional Probability of white given small
p_white_small/p_small

## Probability of white AND big
p_white_big <- 21/100

## Probability of big
p_big <- 63/100

## Conditional Probability of white given big
p_white_big/p_big

## Probability of black AND small
p_black_small <- 10/100

## Conditional probability of black give small
p_black_small/p_small

## Probability of black AND big
p_black_big <- 42/100

## Conditional probability of black given big
p_black_big/p_big
```

### 3. Creating Simple Function to Predict Marble Color

Below find the simple function that predicts the color of marble based on the size. It is a very simple function that predicts marbles will be small if they are white and big if they are black.

```{r}

#' Simple Marble Color Predictor
#'
#' @param x a numeric vector of sizes of marbles 
#'
#' @return a character vector of predictions of marble color based on their size
#'
#'
#' @examples color_predict(marbles_train$size)
color_predict <- function(x) {
  x <- as_tibble(x) %>%
   mutate(predictions = case_when(
      x == "small" ~ "White",
      x == "big" ~ "Black"
    )
  )%>%
    select(predictions)
  return(x)
}

## Applying to Testing Data
color_predict(marbles_test$size)

```
